{
  "source": "Rustonomicon",
  "source_repo": "https://github.com/rust-lang/nomicon",
  "extraction_date": "2026-01-03",
  "chapter": 11,
  "title": "Implementing Arc and Mutex",
  "file": "./arc-mutex/arc-and-mutex.md",
  "sections": [
    {
      "id": "nomicon_implementing_arc_and_mutex",
      "title": "Implementing Arc and Mutex",
      "level": 1,
      "content": "Knowing the theory is all fine and good, but the *best* way to understand\nsomething is to use it. To better understand atomics and interior mutability,\nwe'll be implementing versions of the standard library's `Arc` and `Mutex` types.\n\nTODO: Write `Mutex` chapters.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_implementing_arc_and_mutex_p1": "Knowing the theory is all fine and good, but the *best* way to understand\nsomething is to use it. To better understand atomics and interior mutability,\nwe'll be implementing versions of the standard library's `Arc` and `Mutex` types.",
        "nomicon_implementing_arc_and_mutex_p2": "TODO: Write `Mutex` chapters."
      }
    },
    {
      "id": "nomicon_implementing_arc",
      "title": "Implementing Arc",
      "level": 1,
      "content": "In this section, we'll be implementing a simpler version of `std::sync::Arc`.\nSimilarly to the implementation of `Vec` we made earlier, we won't be\ntaking advantage of as many optimizations, intrinsics, or unstable code as the\nstandard library may.\n\nThis implementation is loosely based on the standard library's implementation\n(technically taken from `alloc::sync` in 1.49, as that's where it's actually\nimplemented), but it will not support weak references at the moment as they\nmake the implementation slightly more complex.\n\nPlease note that this section is very work-in-progress at the moment.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_implementing_arc_p1": "In this section, we'll be implementing a simpler version of `std::sync::Arc`.\nSimilarly to the implementation of `Vec` we made earlier, we won't be\ntaking advantage of as many optimizations, intrinsics, or unstable code as the\nstandard library may.",
        "nomicon_implementing_arc_p2": "This implementation is loosely based on the standard library's implementation\n(technically taken from `alloc::sync` in 1.49, as that's where it's actually\nimplemented), but it will not support weak references at the moment as they\nmake the implementation slightly more complex.",
        "nomicon_implementing_arc_p3": "Please note that this section is very work-in-progress at the moment."
      }
    },
    {
      "id": "nomicon_layout",
      "title": "Layout",
      "level": 1,
      "content": "Let's start by making the layout for our implementation of `Arc`.\n\nAn `Arc<T>` provides thread-safe shared ownership of a value of type `T`,\nallocated in the heap. Sharing implies immutability in Rust, so we don't need to\ndesign anything that manages access to that value, right? Although interior\nmutability types like Mutex allow Arc's users to create shared mutability, Arc\nitself doesn't need to concern itself with these issues.\n\nHowever there _is_ one place where Arc needs to concern itself with mutation:\ndestruction. When all the owners of the Arc go away, we need to be able to\n`drop` its contents and free its allocation. So we need a way for an owner to\nknow if it's the _last_ owner, and the simplest way to do that is with a count\nof the owners -- Reference Counting.\n\nUnfortunately, this reference count is inherently shared mutable state, so Arc\n_does_ need to think about synchronization. We _could_ use a Mutex for this, but\nthat's overkill. Instead, we'll use atomics. And since everyone already needs a\npointer to the T's allocation, we might as well put the reference count in that\nsame allocation.\n\nNaively, it would look something like this:\n\nuse std::sync::atomic;\n\npub struct Arc<T> {\n    ptr: *mut ArcInner<T>,\n}\n\npub struct ArcInner<T> {\n    rc: atomic::AtomicUsize,\n    data: T,\n}\n\nThis would compile, however it would be incorrect. First of all, the compiler\nwill give us too strict variance. For example, an `Arc<&'static str>` couldn't\nbe used where an `Arc<&'a str>` was expected. More importantly, it will give\nincorrect ownership information to the drop checker, as it will assume we don't\nown any values of type `T`. As this is a structure providing shared ownership of\na value, at some point there will be an instance of this structure that entirely\nowns its data. See the chapter on ownership and lifetimes for\nall the details on variance and drop check.\n\nTo fix the first problem, we can use `NonNull<T>`. Note that `NonNull<T>` is a\nwrapper around a raw pointer that declares that:\n\n* We are covariant over `T`\n* Our pointer is never null\n\nTo fix the second problem, we can include a `PhantomData` marker containing an\n`ArcInner<T>`. This will tell the drop checker that we have some notion of\nownership of a value of `ArcInner<T>` (which itself contains some `T`).\n\nWith these changes we get our final structure:\n\nuse std::marker::PhantomData;\nuse std::ptr::NonNull;\nuse std::sync::atomic::AtomicUsize;\n\npub struct Arc<T> {\n    ptr: NonNull<ArcInner<T>>,\n    phantom: PhantomData<ArcInner<T>>,\n}\n\npub struct ArcInner<T> {\n    rc: AtomicUsize,\n    data: T,\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_layout_p1": "Let's start by making the layout for our implementation of `Arc`.",
        "nomicon_layout_p2": "An `Arc<T>` provides thread-safe shared ownership of a value of type `T`,\nallocated in the heap. Sharing implies immutability in Rust, so we don't need to\ndesign anything that manages access to that value, right? Although interior\nmutability types like Mutex allow Arc's users to create shared mutability, Arc\nitself doesn't need to concern itself with these issues.",
        "nomicon_layout_p3": "However there _is_ one place where Arc needs to concern itself with mutation:\ndestruction. When all the owners of the Arc go away, we need to be able to\n`drop` its contents and free its allocation. So we need a way for an owner to\nknow if it's the _last_ owner, and the simplest way to do that is with a count\nof the owners -- Reference Counting.",
        "nomicon_layout_p4": "Unfortunately, this reference count is inherently shared mutable state, so Arc\n_does_ need to think about synchronization. We _could_ use a Mutex for this, but\nthat's overkill. Instead, we'll use atomics. And since everyone already needs a\npointer to the T's allocation, we might as well put the reference count in that\nsame allocation.",
        "nomicon_layout_p5": "Naively, it would look something like this:",
        "nomicon_layout_p6": "use std::sync::atomic;",
        "nomicon_layout_p7": "pub struct Arc<T> {\n    ptr: *mut ArcInner<T>,\n}",
        "nomicon_layout_p8": "pub struct ArcInner<T> {\n    rc: atomic::AtomicUsize,\n    data: T,\n}",
        "nomicon_layout_p9": "This would compile, however it would be incorrect. First of all, the compiler\nwill give us too strict variance. For example, an `Arc<&'static str>` couldn't\nbe used where an `Arc<&'a str>` was expected. More importantly, it will give\nincorrect ownership information to the drop checker, as it will assume we don't\nown any values of type `T`. As this is a structure providing shared ownership of\na value, at some point there will be an instance of this structure that entirely\nowns its data. See the chapter on ownership and lifetimes for\nall the details on variance and drop check.",
        "nomicon_layout_p10": "To fix the first problem, we can use `NonNull<T>`. Note that `NonNull<T>` is a\nwrapper around a raw pointer that declares that:",
        "nomicon_layout_p11": "* We are covariant over `T`\n* Our pointer is never null",
        "nomicon_layout_p12": "To fix the second problem, we can include a `PhantomData` marker containing an\n`ArcInner<T>`. This will tell the drop checker that we have some notion of\nownership of a value of `ArcInner<T>` (which itself contains some `T`).",
        "nomicon_layout_p13": "With these changes we get our final structure:",
        "nomicon_layout_p14": "use std::marker::PhantomData;\nuse std::ptr::NonNull;\nuse std::sync::atomic::AtomicUsize;",
        "nomicon_layout_p15": "pub struct Arc<T> {\n    ptr: NonNull<ArcInner<T>>,\n    phantom: PhantomData<ArcInner<T>>,\n}",
        "nomicon_layout_p16": "pub struct ArcInner<T> {\n    rc: AtomicUsize,\n    data: T,\n}"
      }
    },
    {
      "id": "nomicon_base_code",
      "title": "Base Code",
      "level": 1,
      "content": "Now that we've decided the layout for our implementation of `Arc`, let's create\nsome basic code.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_base_code_p1": "Now that we've decided the layout for our implementation of `Arc`, let's create\nsome basic code."
      }
    },
    {
      "id": "nomicon_constructing_the_arc",
      "title": "Constructing the Arc",
      "level": 2,
      "content": "We'll first need a way to construct an `Arc<T>`.\n\nThis is pretty simple, as we just need to box the `ArcInner<T>` and get a\n`NonNull<T>` pointer to it.\n\n<!-- ignore: simplified code -->\n,ignore\nimpl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_constructing_the_arc_p1": "We'll first need a way to construct an `Arc<T>`.",
        "nomicon_constructing_the_arc_p2": "This is pretty simple, as we just need to box the `ArcInner<T>` and get a\n`NonNull<T>` pointer to it.",
        "nomicon_constructing_the_arc_p3": "<!-- ignore: simplified code -->\n,ignore\nimpl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}"
      }
    },
    {
      "id": "nomicon_send_and_sync",
      "title": "Send and Sync",
      "level": 2,
      "content": "Since we're building a concurrency primitive, we'll need to be able to send it\nacross threads. Thus, we can implement the `Send` and `Sync` marker traits. For\nmore information on these, see the section on `Send` and\n`Sync`.\n\nThis is okay because:\n* You can only get a mutable reference to the value inside an `Arc` if and only\n  if it is the only `Arc` referencing that data (which only happens in `Drop`)\n* We use atomics for the shared mutable reference counting\n\n<!-- ignore: simplified code -->\n,ignore\nunsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}\n\nWe need to have the bound `T: Sync + Send` because if we did not provide those\nbounds, it would be possible to share values that are thread-unsafe across a\nthread boundary via an `Arc`, which could possibly cause data races or\nunsoundness.\n\nFor example, if those bounds were not present, `Arc<Rc<u32>>` would be `Sync` or\n`Send`, meaning that you could clone the `Rc` out of the `Arc` to send it across\na thread (without creating an entirely new `Rc`), which would create data races\nas `Rc` is not thread-safe.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_send_and_sync_p1": "Since we're building a concurrency primitive, we'll need to be able to send it\nacross threads. Thus, we can implement the `Send` and `Sync` marker traits. For\nmore information on these, see the section on `Send` and\n`Sync`.",
        "nomicon_send_and_sync_p2": "This is okay because:\n* You can only get a mutable reference to the value inside an `Arc` if and only\n  if it is the only `Arc` referencing that data (which only happens in `Drop`)\n* We use atomics for the shared mutable reference counting",
        "nomicon_send_and_sync_p3": "<!-- ignore: simplified code -->\n,ignore\nunsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}",
        "nomicon_send_and_sync_p4": "We need to have the bound `T: Sync + Send` because if we did not provide those\nbounds, it would be possible to share values that are thread-unsafe across a\nthread boundary via an `Arc`, which could possibly cause data races or\nunsoundness.",
        "nomicon_send_and_sync_p5": "For example, if those bounds were not present, `Arc<Rc<u32>>` would be `Sync` or\n`Send`, meaning that you could clone the `Rc` out of the `Arc` to send it across\na thread (without creating an entirely new `Rc`), which would create data races\nas `Rc` is not thread-safe."
      }
    },
    {
      "id": "nomicon_getting_the_arcinner",
      "title": "Getting the `ArcInner`",
      "level": 2,
      "content": "To dereference the `NonNull<T>` pointer into a `&T`, we can call\n`NonNull::as_ref`. This is unsafe, unlike the typical `as_ref` function, so we\nmust call it like this:\n\n<!-- ignore: simplified code -->\n,ignore\nunsafe { self.ptr.as_ref() }\n\nWe'll be using this snippet a few times in this code (usually with an associated\n`let` binding).\n\nThis unsafety is okay because while this `Arc` is alive, we're guaranteed that\nthe inner pointer is valid.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_getting_the_arcinner_p1": "To dereference the `NonNull<T>` pointer into a `&T`, we can call\n`NonNull::as_ref`. This is unsafe, unlike the typical `as_ref` function, so we\nmust call it like this:",
        "nomicon_getting_the_arcinner_p2": "<!-- ignore: simplified code -->\n,ignore\nunsafe { self.ptr.as_ref() }",
        "nomicon_getting_the_arcinner_p3": "We'll be using this snippet a few times in this code (usually with an associated\n`let` binding).",
        "nomicon_getting_the_arcinner_p4": "This unsafety is okay because while this `Arc` is alive, we're guaranteed that\nthe inner pointer is valid."
      }
    },
    {
      "id": "nomicon_deref",
      "title": "Deref",
      "level": 2,
      "content": "Alright. Now we can make `Arc`s (and soon will be able to clone and destroy them correctly), but how do we get\nto the data inside?\n\nWhat we need now is an implementation of `Deref`.\n\nWe'll need to import the trait:\n\n<!-- ignore: simplified code -->\n,ignore\nuse std::ops::Deref;\n\nAnd here's the implementation:\n\n<!-- ignore: simplified code -->\n,ignore\nimpl<T> Deref for Arc<T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}\n\nPretty simple, eh? This simply dereferences the `NonNull` pointer to the\n`ArcInner<T>`, then gets a reference to the data inside.",
      "parent_id": null,
      "paragraphs": {
        "nomicon_deref_p1": "Alright. Now we can make `Arc`s (and soon will be able to clone and destroy them correctly), but how do we get\nto the data inside?",
        "nomicon_deref_p2": "What we need now is an implementation of `Deref`.",
        "nomicon_deref_p3": "We'll need to import the trait:",
        "nomicon_deref_p4": "<!-- ignore: simplified code -->\n,ignore\nuse std::ops::Deref;",
        "nomicon_deref_p5": "And here's the implementation:",
        "nomicon_deref_p6": "<!-- ignore: simplified code -->\n,ignore\nimpl<T> Deref for Arc<T> {\n    type Target = T;",
        "nomicon_deref_p7": "fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}",
        "nomicon_deref_p8": "Pretty simple, eh? This simply dereferences the `NonNull` pointer to the\n`ArcInner<T>`, then gets a reference to the data inside."
      }
    },
    {
      "id": "nomicon_code",
      "title": "Code",
      "level": 2,
      "content": "Here's all the code from this section:\n\n<!-- ignore: simplified code -->\n,ignore\nuse std::ops::Deref;\n\nimpl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}\n\nunsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}\n\nimpl<T> Deref for Arc<T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_code_p1": "Here's all the code from this section:",
        "nomicon_code_p2": "<!-- ignore: simplified code -->\n,ignore\nuse std::ops::Deref;",
        "nomicon_code_p3": "impl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}",
        "nomicon_code_p4": "unsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}",
        "nomicon_code_p5": "impl<T> Deref for Arc<T> {\n    type Target = T;",
        "nomicon_code_p6": "fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}"
      }
    },
    {
      "id": "nomicon_cloning",
      "title": "Cloning",
      "level": 1,
      "content": "Now that we've got some basic code set up, we'll need a way to clone the `Arc`.\n\nBasically, we need to:\n\n1. Increment the atomic reference count\n2. Construct a new instance of the `Arc` from the inner pointer\n\nFirst, we need to get access to the `ArcInner`:\n\n<!-- ignore: simplified code -->\n,ignore\nlet inner = unsafe { self.ptr.as_ref() };\n\nWe can update the atomic reference count as follows:\n\n<!-- ignore: simplified code -->\n,ignore\nlet old_rc = inner.rc.fetch_add(1, Ordering::???);\n\nBut what ordering should we use here? We don't really have any code that will\nneed atomic synchronization when cloning, as we do not modify the internal value\nwhile cloning. Thus, we can use a Relaxed ordering here, which implies no\nhappens-before relationship but is atomic. When `Drop`ping the Arc, however,\nwe'll need to atomically synchronize when decrementing the reference count. This\nis described more in the section on the `Drop` implementation for\n`Arc`. For more information on atomic relationships and Relaxed\nordering, see the section on atomics.\n\nThus, the code becomes this:\n\n<!-- ignore: simplified code -->\n,ignore\nlet old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);\n\nWe'll need to add another import to use `Ordering`:\n\nuse std::sync::atomic::Ordering;\n\nHowever, we have one problem with this implementation right now. What if someone\ndecides to `mem::forget` a bunch of Arcs? The code we have written so far (and\nwill write) assumes that the reference count accurately portrays how many Arcs\nare in memory, but with `mem::forget` this is false. Thus, when more and more\nArcs are cloned from this one without them being `Drop`ped and the reference\ncount being decremented, we can overflow! This will cause use-after-free which\nis **INCREDIBLY BAD!**\n\nTo handle this, we need to check that the reference count does not go over some\narbitrary value (below `usize::MAX`, as we're storing the reference count as an\n`AtomicUsize`), and do *something*.\n\nThe standard library's implementation decides to just abort the program (as it\nis an incredibly unlikely case in normal code and if it happens, the program is\nprobably incredibly degenerate) if the reference count reaches `isize::MAX`\n(about half of `usize::MAX`) on any thread, on the assumption that there are\nprobably not about 2 billion threads (or about **9 quintillion** on some 64-bit\nmachines) incrementing the reference count at once. This is what we'll do.\n\nIt's pretty simple to implement this behavior:\n\n<!-- ignore: simplified code -->\n,ignore\nif old_rc >= isize::MAX as usize {\n    std::process::abort();\n}\n\nThen, we need to return a new instance of the `Arc`:\n\n<!-- ignore: simplified code -->\n,ignore\nSelf {\n    ptr: self.ptr,\n    phantom: PhantomData\n}\n\nNow, let's wrap this all up inside the `Clone` implementation:\n\n<!-- ignore: simplified code -->\n,ignore\nuse std::sync::atomic::Ordering;\n\nimpl<T> Clone for Arc<T> {\n    fn clone(&self) -> Arc<T> {\n        let inner = unsafe { self.ptr.as_ref() };\n        // Using a relaxed ordering is alright here as we don't need any atomic\n        // synchronization here as we're not modifying or accessing the inner\n        // data.\n        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);\n\n        if old_rc >= isize::MAX as usize {\n            std::process::abort();\n        }\n\n        Self {\n            ptr: self.ptr,\n            phantom: PhantomData,\n        }\n    }\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_cloning_p1": "Now that we've got some basic code set up, we'll need a way to clone the `Arc`.",
        "nomicon_cloning_p2": "Basically, we need to:",
        "nomicon_cloning_p3": "1. Increment the atomic reference count\n2. Construct a new instance of the `Arc` from the inner pointer",
        "nomicon_cloning_p4": "First, we need to get access to the `ArcInner`:",
        "nomicon_cloning_p5": "<!-- ignore: simplified code -->\n,ignore\nlet inner = unsafe { self.ptr.as_ref() };",
        "nomicon_cloning_p6": "We can update the atomic reference count as follows:",
        "nomicon_cloning_p7": "<!-- ignore: simplified code -->\n,ignore\nlet old_rc = inner.rc.fetch_add(1, Ordering::???);",
        "nomicon_cloning_p8": "But what ordering should we use here? We don't really have any code that will\nneed atomic synchronization when cloning, as we do not modify the internal value\nwhile cloning. Thus, we can use a Relaxed ordering here, which implies no\nhappens-before relationship but is atomic. When `Drop`ping the Arc, however,\nwe'll need to atomically synchronize when decrementing the reference count. This\nis described more in the section on the `Drop` implementation for\n`Arc`. For more information on atomic relationships and Relaxed\nordering, see the section on atomics.",
        "nomicon_cloning_p9": "Thus, the code becomes this:",
        "nomicon_cloning_p10": "<!-- ignore: simplified code -->\n,ignore\nlet old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);",
        "nomicon_cloning_p11": "We'll need to add another import to use `Ordering`:",
        "nomicon_cloning_p12": "use std::sync::atomic::Ordering;",
        "nomicon_cloning_p13": "However, we have one problem with this implementation right now. What if someone\ndecides to `mem::forget` a bunch of Arcs? The code we have written so far (and\nwill write) assumes that the reference count accurately portrays how many Arcs\nare in memory, but with `mem::forget` this is false. Thus, when more and more\nArcs are cloned from this one without them being `Drop`ped and the reference\ncount being decremented, we can overflow! This will cause use-after-free which\nis **INCREDIBLY BAD!**",
        "nomicon_cloning_p14": "To handle this, we need to check that the reference count does not go over some\narbitrary value (below `usize::MAX`, as we're storing the reference count as an\n`AtomicUsize`), and do *something*.",
        "nomicon_cloning_p15": "The standard library's implementation decides to just abort the program (as it\nis an incredibly unlikely case in normal code and if it happens, the program is\nprobably incredibly degenerate) if the reference count reaches `isize::MAX`\n(about half of `usize::MAX`) on any thread, on the assumption that there are\nprobably not about 2 billion threads (or about **9 quintillion** on some 64-bit\nmachines) incrementing the reference count at once. This is what we'll do.",
        "nomicon_cloning_p16": "It's pretty simple to implement this behavior:",
        "nomicon_cloning_p17": "<!-- ignore: simplified code -->\n,ignore\nif old_rc >= isize::MAX as usize {\n    std::process::abort();\n}",
        "nomicon_cloning_p18": "Then, we need to return a new instance of the `Arc`:",
        "nomicon_cloning_p19": "<!-- ignore: simplified code -->\n,ignore\nSelf {\n    ptr: self.ptr,\n    phantom: PhantomData\n}",
        "nomicon_cloning_p20": "Now, let's wrap this all up inside the `Clone` implementation:",
        "nomicon_cloning_p21": "<!-- ignore: simplified code -->\n,ignore\nuse std::sync::atomic::Ordering;",
        "nomicon_cloning_p22": "impl<T> Clone for Arc<T> {\n    fn clone(&self) -> Arc<T> {\n        let inner = unsafe { self.ptr.as_ref() };\n        // Using a relaxed ordering is alright here as we don't need any atomic\n        // synchronization here as we're not modifying or accessing the inner\n        // data.\n        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);",
        "nomicon_cloning_p23": "if old_rc >= isize::MAX as usize {\n            std::process::abort();\n        }",
        "nomicon_cloning_p24": "Self {\n            ptr: self.ptr,\n            phantom: PhantomData,\n        }\n    }\n}"
      }
    },
    {
      "id": "nomicon_dropping",
      "title": "Dropping",
      "level": 1,
      "content": "We now need a way to decrease the reference count and drop the data once it is\nlow enough, otherwise the data will live forever on the heap.\n\nTo do this, we can implement `Drop`.\n\nBasically, we need to:\n\n1. Decrement the reference count\n2. If there is only one reference remaining to the data, then:\n3. Atomically fence the data to prevent reordering of the use and deletion of\n   the data\n4. Drop the inner data\n\nFirst, we'll need to get access to the `ArcInner`:\n\n<!-- ignore: simplified code -->\n,ignore\nlet inner = unsafe { self.ptr.as_ref() };\n\nNow, we need to decrement the reference count. To streamline our code, we can\nalso return if the returned value from `fetch_sub` (the value of the reference\ncount before decrementing it) is not equal to `1` (which happens when we are not\nthe last reference to the data).\n\n<!-- ignore: simplified code -->\n,ignore\nif inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n    return;\n}\n\nWe then need to create an atomic fence to prevent reordering of the use of the\ndata and deletion of the data. As described in the standard library's\nimplementation of `Arc`:\n> This fence is needed to prevent reordering of use of the data and deletion of\n> the data. Because it is marked `Release`, the decreasing of the reference\n> count synchronizes with this `Acquire` fence. This means that use of the data\n> happens before decreasing the reference count, which happens before this\n> fence, which happens before the deletion of the data.\n>\n> As explained in the Boost documentation,\n>\n> > It is important to enforce any possible access to the object in one\n> > thread (through an existing reference) to *happen before* deleting\n> > the object in a different thread. This is achieved by a \"release\"\n> > operation after dropping a reference (any access to the object\n> > through this reference must obviously happened before), and an\n> > \"acquire\" operation before deleting the object.\n>\n> In particular, while the contents of an Arc are usually immutable, it's\n> possible to have interior writes to something like a `Mutex<T>`. Since a Mutex\n> is not acquired when it is deleted, we can't rely on its synchronization logic\n> to make writes in thread A visible to a destructor running in thread B.\n>\n> Also note that the Acquire fence here could probably be replaced with an\n> Acquire load, which could improve performance in highly-contended situations.\n> See [2].\n>\n> [1]: https://www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html\n> [2]: https://github.com/rust-lang/rust/pull/41714\n[3]: https://github.com/rust-lang/rust/blob/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/alloc/src/sync.rs#L1440-L1467\n\nTo do this, we do the following:",
      "parent_id": null,
      "paragraphs": {
        "nomicon_dropping_p1": "We now need a way to decrease the reference count and drop the data once it is\nlow enough, otherwise the data will live forever on the heap.",
        "nomicon_dropping_p2": "To do this, we can implement `Drop`.",
        "nomicon_dropping_p3": "Basically, we need to:",
        "nomicon_dropping_p4": "1. Decrement the reference count\n2. If there is only one reference remaining to the data, then:\n3. Atomically fence the data to prevent reordering of the use and deletion of\n   the data\n4. Drop the inner data",
        "nomicon_dropping_p5": "First, we'll need to get access to the `ArcInner`:",
        "nomicon_dropping_p6": "<!-- ignore: simplified code -->\n,ignore\nlet inner = unsafe { self.ptr.as_ref() };",
        "nomicon_dropping_p7": "Now, we need to decrement the reference count. To streamline our code, we can\nalso return if the returned value from `fetch_sub` (the value of the reference\ncount before decrementing it) is not equal to `1` (which happens when we are not\nthe last reference to the data).",
        "nomicon_dropping_p8": "<!-- ignore: simplified code -->\n,ignore\nif inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n    return;\n}",
        "nomicon_dropping_p9": "We then need to create an atomic fence to prevent reordering of the use of the\ndata and deletion of the data. As described in the standard library's\nimplementation of `Arc`:\n> This fence is needed to prevent reordering of use of the data and deletion of\n> the data. Because it is marked `Release`, the decreasing of the reference\n> count synchronizes with this `Acquire` fence. This means that use of the data\n> happens before decreasing the reference count, which happens before this\n> fence, which happens before the deletion of the data.\n>\n> As explained in the Boost documentation,\n>\n> > It is important to enforce any possible access to the object in one\n> > thread (through an existing reference) to *happen before* deleting\n> > the object in a different thread. This is achieved by a \"release\"\n> > operation after dropping a reference (any access to the object\n> > through this reference must obviously happened before), and an\n> > \"acquire\" operation before deleting the object.\n>\n> In particular, while the contents of an Arc are usually immutable, it's\n> possible to have interior writes to something like a `Mutex<T>`. Since a Mutex\n> is not acquired when it is deleted, we can't rely on its synchronization logic\n> to make writes in thread A visible to a destructor running in thread B.\n>\n> Also note that the Acquire fence here could probably be replaced with an\n> Acquire load, which could improve performance in highly-contended situations.\n> See [2].\n>\n> [1]: https://www.boost.org/doc/libs/1_55_0/doc/html/atomic/usage_examples.html\n> [2]: https://github.com/rust-lang/rust/pull/41714\n[3]: https://github.com/rust-lang/rust/blob/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/alloc/src/sync.rs#L1440-L1467",
        "nomicon_dropping_p10": "To do this, we do the following:",
        "nomicon_dropping_p11": ""
      }
    },
    {
      "id": "nomicon_use_std_sync_atomic_ordering",
      "title": "use std::sync::atomic::Ordering;",
      "level": 1,
      "content": "use std::sync::atomic;\natomic::fence(Ordering::Acquire);\n\nFinally, we can drop the data itself. We use `Box::from_raw` to drop the boxed\n`ArcInner<T>` and its data. This takes a `*mut T` and not a `NonNull<T>`, so we\nmust convert using `NonNull::as_ptr`.\n\n<!-- ignore: simplified code -->\n,ignore\nunsafe { Box::from_raw(self.ptr.as_ptr()); }\n\nThis is safe as we know we have the last pointer to the `ArcInner` and that its\npointer is valid.\n\nNow, let's wrap this all up inside the `Drop` implementation:\n\n<!-- ignore: simplified code -->\n,ignore\nimpl<T> Drop for Arc<T> {\n    fn drop(&mut self) {\n        let inner = unsafe { self.ptr.as_ref() };\n        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n            return;\n        }\n        // This fence is needed to prevent reordering of the use and deletion\n        // of the data.\n        atomic::fence(Ordering::Acquire);\n        // This is safe as we know we have the last pointer to the `ArcInner`\n        // and that its pointer is valid.\n        unsafe { Box::from_raw(self.ptr.as_ptr()); }\n    }\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_use_std_sync_atomic_ordering_p1": "use std::sync::atomic;\natomic::fence(Ordering::Acquire);",
        "nomicon_use_std_sync_atomic_ordering_p2": "Finally, we can drop the data itself. We use `Box::from_raw` to drop the boxed\n`ArcInner<T>` and its data. This takes a `*mut T` and not a `NonNull<T>`, so we\nmust convert using `NonNull::as_ptr`.",
        "nomicon_use_std_sync_atomic_ordering_p3": "<!-- ignore: simplified code -->\n,ignore\nunsafe { Box::from_raw(self.ptr.as_ptr()); }",
        "nomicon_use_std_sync_atomic_ordering_p4": "This is safe as we know we have the last pointer to the `ArcInner` and that its\npointer is valid.",
        "nomicon_use_std_sync_atomic_ordering_p5": "Now, let's wrap this all up inside the `Drop` implementation:",
        "nomicon_use_std_sync_atomic_ordering_p6": "<!-- ignore: simplified code -->\n,ignore\nimpl<T> Drop for Arc<T> {\n    fn drop(&mut self) {\n        let inner = unsafe { self.ptr.as_ref() };\n        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n            return;\n        }\n        // This fence is needed to prevent reordering of the use and deletion\n        // of the data.\n        atomic::fence(Ordering::Acquire);\n        // This is safe as we know we have the last pointer to the `ArcInner`\n        // and that its pointer is valid.\n        unsafe { Box::from_raw(self.ptr.as_ptr()); }\n    }\n}"
      }
    },
    {
      "id": "nomicon_final_code",
      "title": "Final Code",
      "level": 1,
      "content": "Here's the final code, with some added comments and re-ordered imports:\n\nuse std::marker::PhantomData;\nuse std::ops::Deref;\nuse std::ptr::NonNull;\nuse std::sync::atomic::{self, AtomicUsize, Ordering};\n\npub struct Arc<T> {\n    ptr: NonNull<ArcInner<T>>,\n    phantom: PhantomData<ArcInner<T>>,\n}\n\npub struct ArcInner<T> {\n    rc: AtomicUsize,\n    data: T,\n}\n\nimpl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}\n\nunsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}\n\nimpl<T> Deref for Arc<T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}\n\nimpl<T> Clone for Arc<T> {\n    fn clone(&self) -> Arc<T> {\n        let inner = unsafe { self.ptr.as_ref() };\n        // Using a relaxed ordering is alright here as we don't need any atomic\n        // synchronization here as we're not modifying or accessing the inner\n        // data.\n        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);\n\n        if old_rc >= isize::MAX as usize {\n            std::process::abort();\n        }\n\n        Self {\n            ptr: self.ptr,\n            phantom: PhantomData,\n        }\n    }\n}\n\nimpl<T> Drop for Arc<T> {\n    fn drop(&mut self) {\n        let inner = unsafe { self.ptr.as_ref() };\n        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n            return;\n        }\n        // This fence is needed to prevent reordering of the use and deletion\n        // of the data.\n        atomic::fence(Ordering::Acquire);\n        // This is safe as we know we have the last pointer to the `ArcInner`\n        // and that its pointer is valid.\n        unsafe { Box::from_raw(self.ptr.as_ptr()); }\n    }\n}",
      "parent_id": null,
      "paragraphs": {
        "nomicon_final_code_p1": "Here's the final code, with some added comments and re-ordered imports:",
        "nomicon_final_code_p2": "use std::marker::PhantomData;\nuse std::ops::Deref;\nuse std::ptr::NonNull;\nuse std::sync::atomic::{self, AtomicUsize, Ordering};",
        "nomicon_final_code_p3": "pub struct Arc<T> {\n    ptr: NonNull<ArcInner<T>>,\n    phantom: PhantomData<ArcInner<T>>,\n}",
        "nomicon_final_code_p4": "pub struct ArcInner<T> {\n    rc: AtomicUsize,\n    data: T,\n}",
        "nomicon_final_code_p5": "impl<T> Arc<T> {\n    pub fn new(data: T) -> Arc<T> {\n        // We start the reference count at 1, as that first reference is the\n        // current pointer.\n        let boxed = Box::new(ArcInner {\n            rc: AtomicUsize::new(1),\n            data,\n        });\n        Arc {\n            // It is okay to call `.unwrap()` here as we get a pointer from\n            // `Box::into_raw` which is guaranteed to not be null.\n            ptr: NonNull::new(Box::into_raw(boxed)).unwrap(),\n            phantom: PhantomData,\n        }\n    }\n}",
        "nomicon_final_code_p6": "unsafe impl<T: Sync + Send> Send for Arc<T> {}\nunsafe impl<T: Sync + Send> Sync for Arc<T> {}",
        "nomicon_final_code_p7": "impl<T> Deref for Arc<T> {\n    type Target = T;",
        "nomicon_final_code_p8": "fn deref(&self) -> &T {\n        let inner = unsafe { self.ptr.as_ref() };\n        &inner.data\n    }\n}",
        "nomicon_final_code_p9": "impl<T> Clone for Arc<T> {\n    fn clone(&self) -> Arc<T> {\n        let inner = unsafe { self.ptr.as_ref() };\n        // Using a relaxed ordering is alright here as we don't need any atomic\n        // synchronization here as we're not modifying or accessing the inner\n        // data.\n        let old_rc = inner.rc.fetch_add(1, Ordering::Relaxed);",
        "nomicon_final_code_p10": "if old_rc >= isize::MAX as usize {\n            std::process::abort();\n        }",
        "nomicon_final_code_p11": "Self {\n            ptr: self.ptr,\n            phantom: PhantomData,\n        }\n    }\n}",
        "nomicon_final_code_p12": "impl<T> Drop for Arc<T> {\n    fn drop(&mut self) {\n        let inner = unsafe { self.ptr.as_ref() };\n        if inner.rc.fetch_sub(1, Ordering::Release) != 1 {\n            return;\n        }\n        // This fence is needed to prevent reordering of the use and deletion\n        // of the data.\n        atomic::fence(Ordering::Acquire);\n        // This is safe as we know we have the last pointer to the `ArcInner`\n        // and that its pointer is valid.\n        unsafe { Box::from_raw(self.ptr.as_ptr()); }\n    }\n}"
      }
    }
  ],
  "ids": [
    "nomicon_cloning_p4",
    "nomicon_layout_p14",
    "nomicon_deref_p6",
    "nomicon_use_std_sync_atomic_ordering_p5",
    "nomicon_cloning_p5",
    "nomicon_dropping_p2",
    "nomicon_dropping",
    "nomicon_code_p2",
    "nomicon_final_code_p10",
    "nomicon_deref_p2",
    "nomicon_send_and_sync_p5",
    "nomicon_cloning_p3",
    "nomicon_use_std_sync_atomic_ordering_p1",
    "nomicon_cloning_p14",
    "nomicon_cloning_p16",
    "nomicon_implementing_arc_p3",
    "nomicon_final_code_p7",
    "nomicon_constructing_the_arc",
    "nomicon_send_and_sync_p4",
    "nomicon_deref_p4",
    "nomicon_code_p5",
    "nomicon_final_code_p4",
    "nomicon_dropping_p1",
    "nomicon_layout_p2",
    "nomicon_dropping_p8",
    "nomicon_use_std_sync_atomic_ordering_p6",
    "nomicon_final_code_p1",
    "nomicon_final_code_p5",
    "nomicon_use_std_sync_atomic_ordering_p2",
    "nomicon_final_code_p8",
    "nomicon_final_code_p11",
    "nomicon_deref_p5",
    "nomicon_layout_p6",
    "nomicon_code_p4",
    "nomicon_dropping_p7",
    "nomicon_layout_p4",
    "nomicon_cloning_p17",
    "nomicon_layout_p12",
    "nomicon_dropping_p11",
    "nomicon_send_and_sync_p1",
    "nomicon_cloning_p7",
    "nomicon_deref_p7",
    "nomicon_code",
    "nomicon_getting_the_arcinner_p1",
    "nomicon_deref",
    "nomicon_dropping_p4",
    "nomicon_cloning_p6",
    "nomicon_cloning_p9",
    "nomicon_dropping_p3",
    "nomicon_layout_p15",
    "nomicon_getting_the_arcinner_p4",
    "nomicon_cloning_p10",
    "nomicon_use_std_sync_atomic_ordering",
    "nomicon_final_code_p3",
    "nomicon_layout_p8",
    "nomicon_send_and_sync",
    "nomicon_final_code_p6",
    "nomicon_cloning_p13",
    "nomicon_final_code_p2",
    "nomicon_layout_p13",
    "nomicon_dropping_p5",
    "nomicon_cloning_p23",
    "nomicon_final_code",
    "nomicon_implementing_arc_p2",
    "nomicon_constructing_the_arc_p1",
    "nomicon_base_code_p1",
    "nomicon_constructing_the_arc_p2",
    "nomicon_cloning_p8",
    "nomicon_code_p1",
    "nomicon_cloning_p12",
    "nomicon_send_and_sync_p3",
    "nomicon_layout_p3",
    "nomicon_implementing_arc_and_mutex_p2",
    "nomicon_dropping_p6",
    "nomicon_dropping_p10",
    "nomicon_final_code_p9",
    "nomicon_cloning_p15",
    "nomicon_layout_p7",
    "nomicon_constructing_the_arc_p3",
    "nomicon_deref_p3",
    "nomicon_deref_p8",
    "nomicon_dropping_p9",
    "nomicon_cloning_p1",
    "nomicon_layout_p10",
    "nomicon_layout",
    "nomicon_layout_p1",
    "nomicon_layout_p16",
    "nomicon_base_code",
    "nomicon_cloning_p24",
    "nomicon_implementing_arc_and_mutex_p1",
    "nomicon_cloning_p21",
    "nomicon_implementing_arc_p1",
    "nomicon_cloning",
    "nomicon_cloning_p11",
    "nomicon_cloning_p2",
    "nomicon_cloning_p18",
    "nomicon_send_and_sync_p2",
    "nomicon_cloning_p20",
    "nomicon_use_std_sync_atomic_ordering_p3",
    "nomicon_implementing_arc_and_mutex",
    "nomicon_layout_p11",
    "nomicon_getting_the_arcinner_p2",
    "nomicon_code_p3",
    "nomicon_getting_the_arcinner",
    "nomicon_code_p6",
    "nomicon_layout_p9",
    "nomicon_layout_p5",
    "nomicon_cloning_p22",
    "nomicon_use_std_sync_atomic_ordering_p4",
    "nomicon_implementing_arc",
    "nomicon_deref_p1",
    "nomicon_cloning_p19",
    "nomicon_final_code_p12",
    "nomicon_getting_the_arcinner_p3"
  ]
}
